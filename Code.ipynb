{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicaharutyunyan/Machine_Learning_Foundation-/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ONQmX4ubV2mg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "\n",
        "from typing import Callable, Dict, Tuple, List\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "# GRAPHS_IMG_FILEPATH = \"/Users/seth/development/01_deep-learning-from-scratch/images/02_fundamentals/graphs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crdji9vDV2mj",
        "outputId": "2fec7963-e517-4b98-c34d-726ce6633f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "drTyeyegV2mk"
      },
      "outputs": [],
      "source": [
        "TEST_ALL = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmojGmuYV2mk"
      },
      "source": [
        "# Boston data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DZRhhG0JV2ml",
        "outputId": "d9fa1945-3176-4368-c4e6-3732eb9fea75"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1ed36436ffa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \"\"\"\n\u001b[1;32m    155\u001b[0m         )\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L08_esVJV2mm"
      },
      "outputs": [],
      "source": [
        "boston = load_boston()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Os9hhFlXV2mm"
      },
      "outputs": [],
      "source": [
        "data = boston.data\n",
        "target = boston.target\n",
        "features = boston.feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4NWKmnV2mn"
      },
      "source": [
        "# SciKit Learn Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCfyJ63PV2mo"
      },
      "source": [
        "## Data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "RbxMZY4yV2mo",
        "outputId": "dac6d28b-c23d-4e80-eb0a-b914d877ab75"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7fb1658bf86a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "s = StandardScaler()\n",
        "data = s.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "T-OXlSYEV2mp",
        "outputId": "5fa8b22a-44bd-42b6-9773-6c499feff5f9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9228db385293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80718\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
        "\n",
        "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YyhYpbmV2mp"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "l8_8GuIqV2mp",
        "outputId": "17f5396f-7583-4666-ce1d-52931c7f1e99"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c3fdbca3dc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "lr.fit(X_train, y_train)\n",
        "preds = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL2-bTbZV2mq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1qJtBY1V2mq"
      },
      "outputs": [],
      "source": [
        "plt.xlabel(\"Predicted value\")\n",
        "plt.ylabel(\"Actual value\")\n",
        "plt.title(\"Predicted vs. Actual values for\\nnLinear Regression model\")\n",
        "plt.xlim([0, 51])\n",
        "plt.ylim([0, 51])\n",
        "plt.scatter(preds, y_test)\n",
        "plt.plot([0, 51], [0, 51]);\n",
        "# plt.savefig(IMG_FILEPATH + \"00_linear_real_pred_vs_actual.png\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ9aD1DZV2mr"
      },
      "source": [
        "load_di Testing changing feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ8IVl0rV2mr"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:, 12], y_test)\n",
        "plt.xlabel(\"Most important feature from our data\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Relationship between most\\nimportant feature and target\");\n",
        "# plt.savefig(IMG_FILEPATH + \"02_most_important_feature_vs_target.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5-5eTPBV2mr"
      },
      "source": [
        "Non linear relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVcuEzCV2ms"
      },
      "source": [
        "## Model error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdevvIwKV2ms"
      },
      "outputs": [],
      "source": [
        "def mae(preds: ndarray, actuals: ndarray):\n",
        "    '''\n",
        "    Compute mean absolute error.\n",
        "    '''\n",
        "    return np.mean(np.abs(preds - actuals))\n",
        "\n",
        "def rmse(preds: ndarray, actuals: ndarray):\n",
        "    '''\n",
        "    Compute root mean squared error.\n",
        "    '''\n",
        "    return np.sqrt(np.mean(np.power(preds - actuals, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOnZIZmnV2ms"
      },
      "outputs": [],
      "source": [
        "print(\"Mean absolute error:\", round(mae(preds, y_test), 4), \"\\n\"\n",
        "      \"Root mean squared error:\", round(rmse(preds, y_test), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pg2Xwii7V2ms"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.round(lr.coef_, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k12OabrsV2mt"
      },
      "source": [
        "## Manual linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoEVa5fZV2mt"
      },
      "outputs": [],
      "source": [
        "def forward_linear_regression(X_batch: ndarray,\n",
        "                              y_batch: ndarray,\n",
        "                              weights: Dict[str, ndarray]\n",
        "                              )-> Tuple[float, Dict[str, ndarray]]:\n",
        "    '''\n",
        "    Forward pass for the step-by-step linear regression.\n",
        "    '''\n",
        "    # assert batch sizes of X and y are equal\n",
        "    assert X_batch.shape[0] == y_batch.shape[0]\n",
        "\n",
        "    # assert that matrix multiplication can work\n",
        "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
        "\n",
        "    # assert that B is simply a 1x1 ndarray\n",
        "    assert weights['B'].shape[0] == weights['B'].shape[1] == 1\n",
        "\n",
        "    # compute the operations on the forward pass\n",
        "    N = np.dot(X_batch, weights['W'])\n",
        "\n",
        "    P = N + weights['B']\n",
        "\n",
        "    loss = np.mean(np.power(y_batch - P, 2))\n",
        "\n",
        "    # save the information computed on the forward pass\n",
        "    forward_info: Dict[str, ndarray] = {}\n",
        "    forward_info['X'] = X_batch\n",
        "    forward_info['N'] = N\n",
        "    forward_info['P'] = P\n",
        "    forward_info['y'] = y_batch\n",
        "\n",
        "    return loss, forward_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKmmTNX3V2mt"
      },
      "outputs": [],
      "source": [
        "def to_2d_np(a: ndarray, \n",
        "             type: str = \"col\") -> ndarray:\n",
        "    '''\n",
        "    Turns a 1D Tensor into 2D\n",
        "    '''\n",
        "\n",
        "    assert a.ndim == 1, \\\n",
        "    \"Input tensors must be 1 dimensional\"\n",
        "    \n",
        "    if type == \"col\":        \n",
        "        return a.reshape(-1, 1)\n",
        "    elif type == \"row\":\n",
        "        return a.reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXiqd-jdV2mu"
      },
      "outputs": [],
      "source": [
        "def permute_data(X: ndarray, y: ndarray):\n",
        "    '''\n",
        "    Permute X and y, using the same permutation, along axis=0\n",
        "    '''\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    return X[perm], y[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf_LhvzrV2mu"
      },
      "outputs": [],
      "source": [
        "def loss_gradients(forward_info: Dict[str, ndarray],\n",
        "                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
        "    '''\n",
        "    Compute dLdW and dLdB for the step-by-step linear regression model.\n",
        "    '''\n",
        "    batch_size = forward_info['X'].shape[0]\n",
        "\n",
        "    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n",
        "\n",
        "    dPdN = np.ones_like(forward_info['N'])\n",
        "\n",
        "    dPdB = np.ones_like(weights['B'])\n",
        "\n",
        "    dLdN = dLdP * dPdN\n",
        "\n",
        "    dNdW = np.transpose(forward_info['X'], (1, 0))\n",
        "    \n",
        "    # need to use matrix multiplication here,\n",
        "    # with dNdW on the left (see note at the end of last chapter)    \n",
        "    dLdW = np.dot(dNdW, dLdN)\n",
        "\n",
        "    # need to sum along dimension representing the batch size:\n",
        "    # see note near the end of the chapter    \n",
        "    dLdB = (dLdP * dPdB).sum(axis=0)\n",
        "\n",
        "    loss_gradients: Dict[str, ndarray] = {}\n",
        "    loss_gradients['W'] = dLdW\n",
        "    loss_gradients['B'] = dLdB\n",
        "\n",
        "    return loss_gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAxghCV3V2mu"
      },
      "outputs": [],
      "source": [
        "Batch = Tuple[ndarray, ndarray]\n",
        "\n",
        "def generate_batch(X: ndarray, \n",
        "                   y: ndarray,\n",
        "                   start: int = 0,\n",
        "                   batch_size: int = 10) -> Batch:\n",
        "    '''\n",
        "    Generate batch from X and y, given a start position\n",
        "    '''\n",
        "    assert X.ndim == y.ndim == 2, \\\n",
        "    \"X and Y must be 2 dimensional\"\n",
        "\n",
        "    if start+batch_size > X.shape[0]:\n",
        "        batch_size = X.shape[0] - start\n",
        "    \n",
        "    X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n",
        "    \n",
        "    return X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ss6LVdV2mu"
      },
      "outputs": [],
      "source": [
        "def forward_loss(X: ndarray,\n",
        "                 y: ndarray,\n",
        "                 weights: Dict[str, ndarray]) -> Tuple[Dict[str, ndarray], float]:\n",
        "    '''\n",
        "    Generate predictions and calculate loss for a step-by-step linear regression\n",
        "    (used mostly during inference).\n",
        "    '''\n",
        "    N = np.dot(X, weights['W'])\n",
        "\n",
        "    P = N + weights['B']\n",
        "\n",
        "    loss = np.mean(np.power(y - P, 2))\n",
        "\n",
        "    forward_info: Dict[str, ndarray] = {}\n",
        "    forward_info['X'] = X\n",
        "    forward_info['N'] = N\n",
        "    forward_info['P'] = P\n",
        "    forward_info['y'] = y\n",
        "\n",
        "    return forward_info, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxCUUbxNV2mv"
      },
      "outputs": [],
      "source": [
        "def init_weights(n_in: int) -> Dict[str, ndarray]:\n",
        "    '''\n",
        "    Initialize weights on first forward pass of model.\n",
        "    '''\n",
        "    \n",
        "    weights: Dict[str, ndarray] = {}\n",
        "    W = np.random.randn(n_in, 1)\n",
        "    B = np.random.randn(1, 1)\n",
        "    \n",
        "    weights['W'] = W\n",
        "    weights['B'] = B\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3sVjciOV2mv"
      },
      "outputs": [],
      "source": [
        "def train(X: ndarray, \n",
        "          y: ndarray, \n",
        "          n_iter: int = 1000,\n",
        "          learning_rate: float = 0.01,\n",
        "          batch_size: int = 100,\n",
        "          return_losses: bool = False, \n",
        "          return_weights: bool = False, \n",
        "          seed: int = 1) -> None:\n",
        "    '''\n",
        "    Train model for a certain number of epochs.\n",
        "    '''\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "    start = 0\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = init_weights(X.shape[1])\n",
        "\n",
        "    # Permute data\n",
        "    X, y = permute_data(X, y)\n",
        "    \n",
        "    if return_losses:\n",
        "        losses = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        # Generate batch\n",
        "        if start >= X.shape[0]:\n",
        "            X, y = permute_data(X, y)\n",
        "            start = 0\n",
        "        \n",
        "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
        "        start += batch_size\n",
        "    \n",
        "        # Train net using generated batch\n",
        "        forward_info, loss = forward_loss(X_batch, y_batch, weights)\n",
        "\n",
        "        if return_losses:\n",
        "            losses.append(loss)\n",
        "\n",
        "        loss_grads = loss_gradients(forward_info, weights)\n",
        "        for key in weights.keys():\n",
        "            weights[key] -= learning_rate * loss_grads[key]\n",
        "\n",
        "    if return_weights:\n",
        "        return losses, weights\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O31QZ_B3V2mv"
      },
      "outputs": [],
      "source": [
        "train_info = train(X_train, y_train,\n",
        "                   n_iter = 1000,\n",
        "                   learning_rate = 0.001,\n",
        "                   batch_size=23, \n",
        "                   return_losses=True, \n",
        "                   return_weights=True, \n",
        "                   seed=180708)\n",
        "losses = train_info[0]\n",
        "weights = train_info[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q1VbnMR-V2mv"
      },
      "outputs": [],
      "source": [
        "plt.plot(list(range(1000)), losses);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pela9IRuV2mv"
      },
      "outputs": [],
      "source": [
        "def predict(X: ndarray,\n",
        "            weights: Dict[str, ndarray]):\n",
        "    '''\n",
        "    Generate predictions from the step-by-step linear regression model.\n",
        "    '''\n",
        "\n",
        "    N = np.dot(X, weights['W'])\n",
        "\n",
        "    return N + weights['B']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZkyRafBV2mw"
      },
      "outputs": [],
      "source": [
        "preds = predict(X_test, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bkMVCX2V2mw"
      },
      "outputs": [],
      "source": [
        "print(\"Mean absolute error:\", round(mae(preds, y_test), 4), \"\\n\"\n",
        "      \"Root mean squared error:\", round(rmse(preds, y_test), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwFyn0kmV2mw"
      },
      "outputs": [],
      "source": [
        "np.round(y_test.mean(), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "n7O2UcxUV2mw"
      },
      "outputs": [],
      "source": [
        "np.round(rmse(preds, y_test) / y_test.mean(), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCyJaJ5VV2mw"
      },
      "source": [
        "RMSE is 23% on average of y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRhQeKukV2mw"
      },
      "outputs": [],
      "source": [
        "plt.xlabel(\"Predicted value\")\n",
        "plt.ylabel(\"Actual value\")\n",
        "plt.title(\"Predicted vs. Actual values for\\ncustom linear regression model\");\n",
        "plt.xlim([0, 51])\n",
        "plt.ylim([0, 51])\n",
        "plt.scatter(preds, y_test)\n",
        "plt.plot([0, 51], [0, 51]);\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"01_linear_custom_pred_vs_actual.png\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGRtwQadV2mx"
      },
      "outputs": [],
      "source": [
        "NUM = 40\n",
        "a = np.repeat(X_test[:,:-1].mean(axis=0, keepdims=True), NUM, axis=0)\n",
        "b = np.linspace(-1.5, 3.5, NUM).reshape(NUM, 1)\n",
        "\n",
        "test_feature = np.concatenate([a, b], axis=1)\n",
        "test_preds = predict(test_feature, weights)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VSfPXx2V2mx"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:, 12], y_test)\n",
        "plt.plot(np.array(test_feature[:, -1]), test_preds, linewidth=2, c='orange')\n",
        "plt.ylim([6, 51])\n",
        "plt.xlabel(\"Most important feature (normalized)\")\n",
        "plt.ylabel(\"Target/Predictions\")\n",
        "plt.title(\"Most important feature vs. target and predictions,\\n custom linear regression\");\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"03_most_important_feature_vs_predictions.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVK2RCpAV2mx"
      },
      "source": [
        "## Coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoWQtBEXV2mx"
      },
      "outputs": [],
      "source": [
        "np.round(weights['W'].reshape(-1), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b4mIJpoV2mx"
      },
      "outputs": [],
      "source": [
        "np.round(lr.coef_, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzCptJB7V2my"
      },
      "outputs": [],
      "source": [
        "np.round(weights['B'], 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RfjPRA37V2my"
      },
      "outputs": [],
      "source": [
        "np.round(lr.intercept_, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd_LdiltV2my"
      },
      "source": [
        "Coefficients are the same in the SciKit Learn linear regression as in the \"custom\" linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfE-uXkWV2my"
      },
      "source": [
        "## Theoretical relationship between most important feature and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_9d7HsJV2my"
      },
      "outputs": [],
      "source": [
        "NUM = 40\n",
        "a = np.repeat(X_test[:,:-1].mean(axis=0, keepdims=True), NUM, axis=0)\n",
        "b = np.linspace(-1.5, 3.5, NUM).reshape(NUM, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUpT_EkkV2my"
      },
      "outputs": [],
      "source": [
        "test_feature = np.concatenate([a, b], axis=1)\n",
        "preds = predict(test_feature, weights)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NusjTNh3V2my"
      },
      "outputs": [],
      "source": [
        "plt.scatter(np.array(test_feature[:, -1]), np.array(preds))\n",
        "plt.ylim([6, 51])\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"/04_relationship_most_impt_feat_preds.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtdSbptEV2my"
      },
      "source": [
        "# Neural network regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0Tc4Wo7V2mz"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x: ndarray) -> ndarray:\n",
        "    return 1 / (1 + np.exp(-1.0 * x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDhL0ZbxV2mz"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(-5, 5, 0.01),\n",
        "         sigmoid(np.arange(-5, 5, 0.01)));\n",
        "plt.title(\"Sigmoid function plotted from x=-5 to x=5\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"$sigmoid(x)$\");\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"05_sigmoid_function.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBZEW5exV2mz"
      },
      "outputs": [],
      "source": [
        "def init_weights(input_size: int, \n",
        "                 hidden_size: int) -> Dict[str, ndarray]:\n",
        "    '''\n",
        "    Initialize weights during the forward pass for step-by-step neural network model.\n",
        "    '''\n",
        "    weights: Dict[str, ndarray] = {}\n",
        "    weights['W1'] = np.random.randn(input_size, hidden_size)\n",
        "    weights['B1'] = np.random.randn(1, hidden_size)\n",
        "    weights['W2'] = np.random.randn(hidden_size, 1)\n",
        "    weights['B2'] = np.random.randn(1, 1)\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiB2VBCVV2mz"
      },
      "outputs": [],
      "source": [
        "def forward_loss(X: ndarray,\n",
        "                 y: ndarray,\n",
        "                 weights: Dict[str, ndarray]\n",
        "                 ) -> Tuple[Dict[str, ndarray], float]:\n",
        "    '''\n",
        "    Compute the forward pass and the loss for the step-by-step \n",
        "    neural network model.     \n",
        "    '''\n",
        "    M1 = np.dot(X, weights['W1'])\n",
        "\n",
        "    N1 = M1 + weights['B1']\n",
        "\n",
        "    O1 = sigmoid(N1)\n",
        "    \n",
        "    M2 = np.dot(O1, weights['W2'])\n",
        "\n",
        "    P = M2 + weights['B2']    \n",
        "\n",
        "    loss = np.mean(np.power(y - P, 2))\n",
        "\n",
        "    forward_info: Dict[str, ndarray] = {}\n",
        "    forward_info['X'] = X\n",
        "    forward_info['M1'] = M1\n",
        "    forward_info['N1'] = N1\n",
        "    forward_info['O1'] = O1\n",
        "    forward_info['M2'] = M2\n",
        "    forward_info['P'] = P\n",
        "    forward_info['y'] = y\n",
        "\n",
        "    return forward_info, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fHucYDCV2mz"
      },
      "outputs": [],
      "source": [
        "def loss_gradients(forward_info: Dict[str, ndarray], \n",
        "                   weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
        "    '''\n",
        "    Compute the partial derivatives of the loss with respect to each of the parameters in the neural network.\n",
        "    '''    \n",
        "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
        "    \n",
        "    dPdM2 = np.ones_like(forward_info['M2'])\n",
        "\n",
        "    dLdM2 = dLdP * dPdM2\n",
        "  \n",
        "    dPdB2 = np.ones_like(weights['B2'])\n",
        "\n",
        "    dLdB2 = (dLdP * dPdB2).sum(axis=0)\n",
        "    \n",
        "    dM2dW2 = np.transpose(forward_info['O1'], (1, 0))\n",
        "    \n",
        "    dLdW2 = np.dot(dM2dW2, dLdP)\n",
        "\n",
        "    dM2dO1 = np.transpose(weights['W2'], (1, 0)) \n",
        "\n",
        "    dLdO1 = np.dot(dLdM2, dM2dO1)\n",
        "    \n",
        "    dO1dN1 = sigmoid(forward_info['N1']) * (1- sigmoid(forward_info['N1']))\n",
        "    \n",
        "    dLdN1 = dLdO1 * dO1dN1\n",
        "    \n",
        "    dN1dB1 = np.ones_like(weights['B1'])\n",
        "    \n",
        "    dN1dM1 = np.ones_like(forward_info['M1'])\n",
        "    \n",
        "    dLdB1 = (dLdN1 * dN1dB1).sum(axis=0)\n",
        "    \n",
        "    dLdM1 = dLdN1 * dN1dM1\n",
        "    \n",
        "    dM1dW1 = np.transpose(forward_info['X'], (1, 0)) \n",
        "\n",
        "    dLdW1 = np.dot(dM1dW1, dLdM1)\n",
        "\n",
        "    loss_gradients: Dict[str, ndarray] = {}\n",
        "    loss_gradients['W2'] = dLdW2\n",
        "    loss_gradients['B2'] = dLdB2.sum(axis=0)\n",
        "    loss_gradients['W1'] = dLdW1\n",
        "    loss_gradients['B1'] = dLdB1.sum(axis=0)\n",
        "    \n",
        "    return loss_gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PCRaOobV2m0"
      },
      "outputs": [],
      "source": [
        "def predict(X: ndarray, \n",
        "            weights: Dict[str, ndarray]) -> ndarray:\n",
        "    '''\n",
        "    Generate predictions from the step-by-step neural network model. \n",
        "    '''\n",
        "    M1 = np.dot(X, weights['W1'])\n",
        "\n",
        "    N1 = M1 + weights['B1']\n",
        "\n",
        "    O1 = sigmoid(N1)\n",
        "\n",
        "    M2 = np.dot(O1, weights['W2'])\n",
        "\n",
        "    P = M2 + weights['B2']    \n",
        "\n",
        "    return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRKhG4r2V2m0"
      },
      "outputs": [],
      "source": [
        "def train(X_train: ndarray, y_train: ndarray,\n",
        "          X_test: ndarray, y_test: ndarray,\n",
        "          n_iter: int = 1000,\n",
        "          test_every: int = 1000,\n",
        "          learning_rate: float = 0.01,\n",
        "          hidden_size= 13,\n",
        "          batch_size: int = 100,\n",
        "          return_losses: bool = False, \n",
        "          return_weights: bool = False, \n",
        "          return_scores: bool = False,\n",
        "          seed: int = 1) -> None:\n",
        "\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    start = 0\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = init_weights(X_train.shape[1], \n",
        "                           hidden_size=hidden_size)\n",
        "\n",
        "    # Permute data\n",
        "    X_train, y_train = permute_data(X_train, y_train)\n",
        "    \n",
        "\n",
        "    losses = []\n",
        "        \n",
        "    val_scores = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        # Generate batch\n",
        "        if start >= X_train.shape[0]:\n",
        "            X_train, y_train = permute_data(X_train, y_train)\n",
        "            start = 0\n",
        "        \n",
        "        X_batch, y_batch = generate_batch(X_train, y_train, start, batch_size)\n",
        "        start += batch_size\n",
        "    \n",
        "        # Train net using generated batch\n",
        "        forward_info, loss = forward_loss(X_batch, y_batch, weights)\n",
        "\n",
        "        if return_losses:\n",
        "            losses.append(loss)\n",
        "\n",
        "        loss_grads = loss_gradients(forward_info, weights)\n",
        "        for key in weights.keys():\n",
        "            weights[key] -= learning_rate * loss_grads[key]\n",
        "        \n",
        "        if return_scores:\n",
        "            if i % test_every == 0 and i != 0:\n",
        "                preds = predict(X_test, weights)\n",
        "                val_scores.append(r2_score(preds, y_test))\n",
        "\n",
        "    if return_weights:\n",
        "        return losses, weights, val_scores\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPo0cUwZV2m0"
      },
      "outputs": [],
      "source": [
        "if TEST_ALL:\n",
        "    num_iter = 10000\n",
        "    test_every = 1000\n",
        "    train_info = train(X_train, y_train, X_test, y_test,\n",
        "                       n_iter=num_iter,\n",
        "                       test_every = test_every,\n",
        "                       learning_rate = 0.001,\n",
        "                       batch_size=23, \n",
        "                       return_losses=False, \n",
        "                       return_weights=True, \n",
        "                       return_scores=False,\n",
        "                       seed=80718)\n",
        "    losses = train_info[0]\n",
        "    weights = train_info[1]\n",
        "    val_scores = train_info[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YnD00K6aV2m0"
      },
      "outputs": [],
      "source": [
        "if TEST_ALL:\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "    plt.ylim([-1,1])\n",
        "    plt.plot(list(range(int(num_iter / test_every - 1))), val_scores); \n",
        "    plt.xlabel(\"Batches (000s)\")\n",
        "    plt.title(\"Validation Scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKyYWo6PV2m0"
      },
      "source": [
        "### Learning rate tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLrOar0oV2m0"
      },
      "outputs": [],
      "source": [
        "def r2_score_lr(learning_rate):\n",
        "    train_info = train(X_train, y_train, X_test, y_test,\n",
        "                   n_iter=100000,\n",
        "                   test_every = 100000,\n",
        "                   learning_rate = learning_rate,\n",
        "                   batch_size=23, \n",
        "                   return_losses=False, \n",
        "                   return_weights=True, \n",
        "                   return_scores=False,\n",
        "                   seed=80718)\n",
        "    weights = train_info[1]\n",
        "    preds = predict(X_test, weights)\n",
        "    return r2_score(y_test, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG8E6lJtV2m0"
      },
      "outputs": [],
      "source": [
        "if TEST_ALL:\n",
        "    lrs = np.geomspace(1e-2, 1e-6, num=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQQ2J0qYV2m1"
      },
      "outputs": [],
      "source": [
        "if TEST_ALL:\n",
        "    r2s = [r2_score_lr(lr) for lr in lrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s1u2RS1V2m1"
      },
      "outputs": [],
      "source": [
        "if TEST_ALL:\n",
        "    plt.semilogx(lrs, r2s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhDEXYt6V2m1"
      },
      "source": [
        "## Evaluating best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3faHPIvV2m1"
      },
      "outputs": [],
      "source": [
        "train_info = train(X_train, y_train, X_test, y_test,\n",
        "                   n_iter=10000,\n",
        "                   test_every = 1000,\n",
        "                   learning_rate = 0.001,\n",
        "                   batch_size=23, \n",
        "                   return_losses=True, \n",
        "                   return_weights=True, \n",
        "                   return_scores=False,\n",
        "                   seed=180807)\n",
        "losses = train_info[0]\n",
        "weights = train_info[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8GNohzEV2m1"
      },
      "outputs": [],
      "source": [
        "plt.plot(list(range(10000)), losses);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-nR5RtuV2m1"
      },
      "outputs": [],
      "source": [
        "preds = predict(X_test, weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEAEO4XTV2m1"
      },
      "source": [
        "## Investigation of most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_BxjHhV2m1"
      },
      "source": [
        "Most important combinations of features are the two with absolute values of greater than 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-AB4zW3V2m1"
      },
      "outputs": [],
      "source": [
        "weights['W2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsyWzbMV2m2"
      },
      "source": [
        "These are at index 7 and index 9. Here are the combinations themselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jpI2ThCNV2m2"
      },
      "outputs": [],
      "source": [
        "weights['W1'][7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9GqTF7dV2m2"
      },
      "outputs": [],
      "source": [
        "weights['W1'][9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yaSSaohV2m2"
      },
      "outputs": [],
      "source": [
        "print(\"Mean absolute error:\", round(mae(preds, y_test), 4), \"\\n\"\n",
        "      \"Root mean squared error:\", round(rmse(preds, y_test), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lvHpEu0V2m2"
      },
      "outputs": [],
      "source": [
        "plt.xlabel(\"Predicted value\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Predicted value vs. target,\\n neural network regression\")\n",
        "plt.xlim([0, 51])\n",
        "plt.ylim([0, 51])\n",
        "plt.scatter(preds, y_test)\n",
        "plt.plot([0, 51], [0, 51]);\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"07_neural_network_regression_preds_vs_target.png\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM1VVDrzV2m2"
      },
      "outputs": [],
      "source": [
        "np.round(np.mean(np.array(np.abs(preds - y_test))), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx4nfZvhV2m2"
      },
      "outputs": [],
      "source": [
        "np.round(np.mean(np.array(np.power(preds - y_test, 2))), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhfIzKzrV2m3"
      },
      "source": [
        "## Theoretical relationship between most important feature and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fOh1LH6V2m5"
      },
      "outputs": [],
      "source": [
        "NUM = 40\n",
        "a = np.repeat(X_test[:,:-1].mean(axis=0, keepdims=True), NUM, axis=0)\n",
        "b = np.linspace(-1.5, 3.5, NUM).reshape(NUM, 1)\n",
        "test_feature = np.concatenate([a, b], axis=1)\n",
        "preds_test = predict(test_feature, weights)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "U_j79svDV2m5"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:, 12], preds)\n",
        "plt.plot(np.array(test_feature[:, -1]), preds_test, linewidth=2, c='orange')\n",
        "plt.ylim([6, 51])\n",
        "plt.xlabel(\"Most important feature (normalized)\")\n",
        "plt.ylabel(\"Target/Predictions\")\n",
        "plt.title(\"Most important feature vs. target and predictions,\\n neural network regression\");\n",
        "# plt.savefig(GRAPHS_IMG_FILEPATH + \"08_neural_network_regression_impt_feat_vs_preds.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}